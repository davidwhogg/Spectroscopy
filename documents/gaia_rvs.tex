\documentclass[11pt]{article}
\usepackage{amsmath}

\newcommand{\given}{\,|\,}
\newcommand{\setof}[1]{\left\{{#1}\right\}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\inverse}[1]{{#1}^{\!-1}}
\newcommand{\best}[1]{\widetilde{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\instrument}[1]{\textsl{#1}}
\newcommand{\Gaia}{\instrument{Gaia}}
\newcommand{\RVS}{\instrument{RVS}}

\begin{document}

\section*{Extracting CTE-clean spectra from the \Gaia\ \RVS}
\noindent
by DWH

\begin{abstract}

The \Gaia~\RVS\ data will suffer from observable distortions (CTE)
caused by radiation damage and the charge traps that are caused
thereby.
The \RVS\ team is creating empirical models of the time-dependent
distortions.
These models are computationally feasible to ``run forward''; indeed
parameter estimation (radial velocities, temperatures, and so on)
based on the \RVS\ will be based on a forward model of the data,
including the CTE.
Here we show that it is possible to extract CTE-undistorted spectra
from either single epochs or else multiple epochs of \RVS\ data using
one of these CTE models.
There is no need to ``undistort'' or ``correct'' the data.
Inasmuch as these extracted spectra and their uncertainty tensors will
be close to sufficient statistics for the raw data, they could be used
for stellar parameter estimation.
\end{abstract}

The \RVS\ obtains, during the \Gaia\ mission, $N$ epochs or
observations $y_n$ on some star.
Each observation $y_n$ is some patch or window of pixels, so it can be
thought of as a $D$-dimensional blob or column vector.
At each epoch $n$, there is a parameterized model of the
\RVS\ instrument such that a True\footnote{I am using ``True'' in
  keeping with my rules at GaiaCal 2014.} spectrum $x_n$ can be
projected into the data space by some function $F$, which has
parameters $\alpha_n$.
These parameters include all the CTE parameters, but also the
astrometric and spectrograph trace/mapping and line-spread and
point-spread function parameters.
That is, the model is
\begin{eqnarray}
y_n &=& F(x_n\given \alpha_n) + \mbox{noise}
\quad,
\end{eqnarray}
where ``noise'' indicates something to be discussed below.
The parameters $\alpha_n$ have a subscript $n$ because the parameters
are presumed to be varying through the mission lifetime; the
parameters $\alpha_n$ are the appropriate parameters for epoch $n$
(that is, interpolated to time $t_n$).

The True spectrum $x_n$ is some high-resolution description of the
flux density from the source as a function of wavelength $\lambda$.
In any implementation, the basis for this description must be chosen.
One example of a basis would be delta functions or top-hats on a grid
of wavelengths, such that the $x_n$ is a vector of flux-density values
on a grid.
Interestingly, everything said here (below) can be extended to a
(literally) infinite basis, with appropriate regularization and some
simple mathematical realizations, but that is beyond the current
scope.
Another good basis would be something that is a set of orthonormal
bases that are either tuned to capture the variations of stars or else
tuned to the information content of \RVS\ data.
However, the pixel-grid basis is such a simple idea (and in tune with
the astronomical \foreign{zeitgeist}), let's stick with that for now.
As for pixel spacing or pixel number, it doesn't matter much, so long
as the spacing is \emph{smaller} than the \RVS\ wavlength resolution
$\Delta\lambda$.
There are some additional good properties in what follows if it
happens that the \RVS\ data $y_n$ should fully resolve (in a Nyquist
sense) the True spectrum $x_n$; this means that the $x_n$ should
have a spacing about as fine as the \RVS\ native pixel scale.

A probabilistic model is a likelihood function and some priors.
In what follows, almost everything that matters is the likelihood
function, although weak priors might be useful to regularize the
fitting or optimization that follows.
Imagining that most \RVS\ spectra obtain tens of photons per pixel, a
Gaussian noise model will be a good approximation.
In this case, the likelihood function is:
\begin{eqnarray}
p(y_n\given x_n, \alpha_n) &=& N(y_n\given \mu_n, C_n)
\\
\mu_n &\equiv& F(x_n\given \alpha_n)
\quad ,
\end{eqnarray}
where $N(x\given m,V)$ is the $D$-dimensional Gaussan pdf given mean
$m$ and variance tensor $V$, and $C_n$ is the relevant noise or
uncertainty tensor (covariance matrix) for data vector $y_n$.  In the
case that the \RVS\ pixels obtain independent noise contributions
(probably not a bad approximation, even in the presence of the CTE
distortion, but I don't know; more in next paragraph), $C_n$ is a
diagonal tensor with pixel variances on the diagonal.

Several issues arise at this stage, and I am not sure what factors are
most relevant.
One is the following: If you are shot-noise dominated, your estimate
of the noise $C_n$ will be a function of the expectation value of
$y_n$, not the observed value of $y_n$.
That is, the $C_n$ depend on the True spectrum $x_n$ and the
parameters $\alpha_n$.
This is no problem in principle, so long as the likelihood function
includes all terms, and not just the term proportional to $\exp
-\chi^2/2$.
Another issue is that the CTE traps probably correlate the photon
noise, so pixels are no longer independent with respect to shot noise.
In detail, it might be possible to write down a generative model of
the CTE-distorted spectra, but \emph{not possible} to write down a
likelihood function at all!
In arbitrarily covariant situations this can become the case; the
truly arbitrary case cannot be described simply with a covariance
matrix.
If this is really the case, the Best Thing To Do is a form of
approximate inference called Approximate Bayesian Computation.
I don't recommend this, and I don't think you are really in this
regime, since you are doing inference on astrophysical parameters
anyway.
If you can do the astrophysical parameter inference properly, then you
can also extract spectra using the same kind of likelihood function.
A third issue is that if the model has light correlations of the
noise, which \emph{can} be modeled with a covariance matrix, you need
to make a model or quantitative description of those.
That is straightforward but not necessarily trivial.

Now the key idea is to \emph{maximize the likelihood} with respect to
the True spectrum $x_n$:
\begin{eqnarray}
\best{x_n} &\leftarrow& \argmax_{x_n} p(y_n\given x_n, \alpha_n)
\quad .
\end{eqnarray}
This $\best{x_n}$ will be the ``best'' True spectrum to explain the
data.
Alternatively, you can ``regularize'' by applying a light prior
(perhaps something that very slighly prefers smooth spectra to spiky
spectra); then you maximize the posterior pdf rather than the
likelihood; this is a \emph{maximum a posteriori} or \emph{maximum
  regularized likelihood} solution.
In either case, the ``extracted spectrum'' for the data $y_n$ is the
$x_n$ that maximizes a scalar objective based on the likelihood
function.

At high signal-to-noise, the likelihood function will look Gaussian
(or the log likelihood will look parabolic).
The inverse-variance tensor $\inverse{C_{xn}}$ uncertainty on the
best-fit spectrum (inverse of the covariance matrix) is just a second
derivative of the log likelihood:
\begin{eqnarray}
\inverse{C_{xn}} &\leftarrow&
\left.\frac{\dd^2}{\dd x_n^2}\ln p(y_n\given x_n, \alpha_n)\right|_{\best{x_n}}
\quad ,
\end{eqnarray}
where $C_{xn}$ is a $K\times K$ non-negative semi-definite matrix or
tensor, and $K$ is the size of the basis in which the True spectra
$x_n$ are represented.
The Gaussian approximation to the likelihood surface will be pretty
good, I expect, in real cases.

This is all a description of how to extract a \emph{single} spectrum
$\best{x_n}$ given a single epoch $y_n$ of data.
If the spectrum is \emph{not variable}, and if there are \emph{no velocity shifts}
from epoch to epoch, and if the Gaussian (parabolic in log)
approximation is good, the extracted spectra can just be combined to
make an overall best spectrum in the usual way:
\begin{eqnarray}
\best{x} &\leftarrow& 
\inverse{\left[\sum_n \inverse{C_{xn}}\right]}\cdot\left[\sum_n \inverse{C_{xn}}\cdot \best{x_n}\right]
\end{eqnarray}
that is, the best True spectrum $\best{x}$ is a tensor weighted
average of all the epochs.

Note that because the 

...If the spectral variability is known...simple models include no
variability or just velocity changes...what do you do?

...In principle $\setof{v_n}$ can be modeled simultaneously with the
$\setof{x_n}$

...What do do if we require marginalization (of an noise model or else
the $\setof{v_n}$) is required?  We can use MCMC so long as we
decorate chains with likelihoods for likelihood function
reconstruction.

In the end, what is proposed here is essentially what must be being
done by the \RVS\ team to get stellar parameters.
The only difference is that the true spectrum $x_n$ has been treated
here as a completely free function of wavelength, whereas in parameter
estimation, the spectrum was generated by some highly constrained
stellar model.
This is not a significant conceptual difference; indeed the inference
used to constrain stellar parameters could in principle happen
\emph{after} the spectrum extraction proposed here.
This would not even be approximate, if the extracted spectra are
``sufficient statistics'' for the raw data.
It is my guess that they are \emph{very close} to being sufficient
statistics, and therefore that the \RVS\ team \emph{could} perform
one-dimensional spectrum extractions as described here, and then
perform all stellar parameter estimation on those extracted spectra
and their associated uncertainty tensors.

\end{document}
