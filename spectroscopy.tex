% to-do
% -----
% - DWH: words about what extraction does and should return
% - DWH: describe model for trace and psf, priors on psf, psf variation, etc

\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,mathrsfs,color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.25}
\hypersetup{
  colorlinks=true,        % false: boxed links; true: colored links
  linkcolor=linkcolor,    % color of internal links
  citecolor=linkcolor,    % color of links to bibliography
  filecolor=linkcolor,    % color of file links
  urlcolor=linkcolor      % color of external links
}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\transpose}[1]{{#1}^{\scriptscriptstyle \top}}
\newcommand{\hmatrix}[1]{\boldsymbol{#1}}
\newcommand{\Amatrix}{\hmatrix{A}}
\newcommand{\zero}{\hmatrix{0}}
\newcommand{\identity}{\hmatrix{0}}
\newcommand{\pixels}{\hmatrix{p}}
\newcommand{\biaspixels}{\hmatrix{p}^\mathrm{(bias)}}
\newcommand{\darkpixels}{\hmatrix{p}^\mathrm{(dark)}}
\newcommand{\arcpixels}{\hmatrix{p}^\mathrm{(arc)}}
\newcommand{\flatpixels}{\hmatrix{p}^\mathrm{(flat)}}
\newcommand{\bias}{\hmatrix{b}}
\newcommand{\dark}{\hmatrix{d}}
\newcommand{\flux}{\hmatrix{f}}
\newcommand{\fluxLSF}{\hmatrix{\tilde{f}}}
\newcommand{\sky}{\hmatrix{s}}
\newcommand{\arc}{\hmatrix{a}}
\newcommand{\lamp}{\hmatrix{q}}
\renewcommand{\angle}{\hmatrix{\theta}}
\newcommand{\error}{\hmatrix{e}}
\newcommand{\pixelvariance}{\hmatrix{\Sigma}}
\newcommand{\covariance}{\hmatrix{C}}
\newcommand{\Normal}{\mathscr{N}}
\newcommand{\exptime}{t}

\begin{document}

\paragraph{Introduction:}
In a modern spectrograph, light from fibers or slits is dispersed onto
a two-dimensional CCD or CCD-like detector, with (usually) one
direction on the detector corresponding to an angular displacement on
the sky and another (usually) close-to-orthogonal direction
corresponding to wavelength.  There are also slitless spectrographs,
where one direction is a mixture of angular position and wavelength.
The problem of spectroscopic data reduction is the problem of
extracting the one-dimensional spectra---astronomical source flux
densities as a function of wavelength---from the two-dimensional
images.  There is a literature on ``optimal extraction'' of these
one-dimensional spectra; the best extraction methods treat the
two-dimensional image pixels as data to be fit by a model that
consists of a one-dimensional spectrum laid out geometrically on the
device and convolved with a two-dimensional point-spread function
(which might be a function of the atmospheric seeing or the device
properties or both).

Although the optimal extraction literature solves some important
problems, the hardest part of spectroscopic data reduction lies not in
the extraction step but in the step of measuring or learning the
geometric and point-spread functions themselves.  These functions have
various names in the literature but they can be expressed with a
single rectangular matrix $\Amatrix$:
\begin{equation}
\Delta\pixels = \exptime\,\Amatrix\,\flux \quad ,
\end{equation}
where the column vector $\pixels$ is a one-dimensional matrix listing
all of the pixel values in the two-dimensional detector in some order,
the column vector $\Delta\pixels$ lists the differential contribution
to those pixels from an individual astronomical object (that is, the
contribution over and above the bias, dark, sky, and any residual
contributions from other spatially separated sources), the scalar
$\exptime$ is the exposure time of the image, and the column vector
$\flux$ is a list of the flux densities on a grid or list of
wavelength values coming from that astronomical object.  In this
description, each element of the matrix $\Amatrix$ gives the rate of
change of a particular pixel in the two-dimensional detector in
response to unit flux density at a particular wavelength from the
source.  For simplicity, we will imagine having fiber or slit spectra
with small spectral apertures so that we don't have to model the
aperture illumination in detail.  In index notation, this relation
could be written
\begin{equation}
\Delta p_k = \exptime\,\sum_{\ell} A_{k\ell}\,f_{\ell} \quad ,
\end{equation}
where the index $k$ goes over image pixels (the two-dimensional array
has been ``unwrapped'' into a one-dimensional list), and the index
$\ell$ goes over wavelengths $\lambda_{\ell}$.

In what follows, we develop a method for inferring this matrix
$\Amatrix$, which contains almost all of the calibration information
necessary for the extraction of spectroscopic data.  Justifiable
objective methods for inferring $\Amatrix$ are not currently in
general use.  Most spectroscopic surveys use a combination of ad-hoc
methods to estimate wavelength solutions, geometric trace functions,
and point spread functions, and combine these to create approximations
to $\Amatrix$.  Our goal is to produce a flexible objective
methodology in which decisions are made by the data not by the
investigators, and that will be extensible to a wide range of
situations.  We will try to keep the discussion general, but where we
need to get specific we will work with the data from the SDSS-III BOSS
spectrographs.

\paragraph{General framework:}
Our generative model for the data---for a science exposure or data
taken of astronomical targets---is that the column vector $\pixels_i$
of pixel values in exposure $i$ can be expressed as a sum of the
following terms:
\begin{equation}
\pixels_i = \bias + \exptime_i\,\dark
          + \exptime_i\,\sum_j \Amatrix_j\,\flux_j
          + \exptime_i\,\sum_j \Amatrix_j\,\sky_i(\angle_j)
          + \error_i \quad ,
\end{equation}
where the column vector $\bias$ lists the bias or zero-level for every
pixel, the scalar $\exptime_i$ is the exposure time for exposure $i$,
the column vector $\dark$ lists the dark current rate for every pixel,
there is a rectangular matrix $\Amatrix_j$ for every spectroscopic
aperture or fiber $j$, the column vector $\flux_j$ lists the flux
density level for every wavelength in the wavelength grid for the
source illuminating aperture $j$, the column vector $\sky_i(\angle_j)$
lists the flux density (intensity integrated over the aperture) of the
sky, which is permitted to be different for different exposures and to
be a weak function of the angular position $\angle_j$ of aperture $j$,
and the column vector $\error_i$ lists the errors or residuals away
from the model in this exposure $i$.  In index notation, this could be
written
\begin{equation}
p_{ik} = b_k + \exptime_i\,d_k
       + \exptime_i\,\sum_{j,\ell} A_{jk\ell}\,f_{j\ell}
       + \exptime_i\,\sum_{j,\ell} A_{jk\ell}\,s_{i\ell}(\angle_j)
       + e_{ik} \quad .
\end{equation}

There are also bias, dark, arc, and flat frames, which in our model
are generated by
\begin{eqnarray}\displaystyle
\biaspixels_i & = & \bias 
                + \error_i \quad ,\\
\darkpixels_i & = & \bias + \exptime_i\,\dark
                + \error_i \quad ,\\
\arcpixels_i  & = & \bias + \exptime_i\,\dark
                + \exptime_i\,\sum_j \Amatrix_j\,\arc(\angle_j)
                + \error_i \quad ,\\
\flatpixels_i & = & \bias + \exptime_i\,\dark
                + \exptime_i\,\sum_j \Amatrix_j\,\lamp(\angle_j)
                + \error_i \quad ,
\end{eqnarray}
where the error vector $\error_i$ will be different for every frame,
presumably drawn from a distribution function that depends on
illumination but is known at some level, the column vector
$\arc(\angle_j)$ lists the flux density values for an arc lamp,
probably modeled as a mixture of narrow emission lines at known
wavelengths, but permitted to vary in intensities slowly with the
angular position $\angle_j$ of aperture $j$, and the column vector
$\lamp(\angle_j)$ lists the flux density values for a flat-field lamp,
probably modeled as a very smooth function or a mixture of
near-blackbodies, and again with slow angular variations.

Each error vector $\error_i$ is assumed to be drawn from a gaussian
distribution with a variance tensor $\pixelvariance_i$ that is
diagonal:
\begin{equation}
p(\error_i) =\Normal(\error_i|\zero,\pixelvariance_i) \quad,
\end{equation}
where $p(\error_i)$ represents a frequency distribution for the error
vector, and $\Normal(x|m,V)$ is the Gaussian or normal distribution
given a mean $m$ and a variance $V$.  The expectation that the
variance tensor $\pixelvariance_i$ be (close to) diagonal is
equivalent to the expectation that the pixel-to-pixel noise
contributions be (close to) independent and uncorrelated.  Each pixel
will have a different individual noise variance, depending on its
particular amplifier characteristics, dark current, and illumination
in the particular exposure, so the diagonal elements of
$\pixelvariance_i$ will all be different in general.  Furthermore, the
variance tensor $\pixelvariance_i$ will be different for different
exposures $i$ because, in general, the illumination and exposure times
will be different.

Our problem is to infer from these data---science exposures, biases,
darks, arcs, and flats---all of the matrices $\Amatrix_j$ for all of
the apertures $j$.

\paragraph{Extraction:}
Although extraction is \emph{not} the primary goal of this project,
but rather the inference of the calibration matrices $\Amatrix_j$, it
is worth saying a few words about optimal extraction.  What we say
here is largely in response to the Bolton \& Schlegel contribution,
which is, to our knowledge, the most sophisticated treatment of the
problem to date.  To paraphrase, in this contribution, they recommend
choosing a wavelength grid such that the matrices
\begin{equation}
\inverse{\covariance_{ij}} \equiv
  \transpose{\Amatrix_j}\,\inverse{\pixelvariance_i}\,\Amatrix_j
\end{equation}
are invertible, computing vectors
\begin{equation}
\fluxLSF_j \equiv
  \inverse{[\transpose{\Amatrix_j}\,\inverse{\pixelvariance_i}\,\Amatrix_j]}
  \,\transpose{\Amatrix_j}\,\inverse{\pixelvariance_i}
  \,\left[\frac{1}{t_i}\,\Delta\pixels_i\right] \quad ,
\end{equation}
where $\Delta\pixels_i$ is a bias-subtracted, dark-subtracted,
sky-subtracted, and possibly overlapping-spectrum-subtracted
spectroscopic image frame.  In detail they do not actually recommend
sky subtraction as a prior step but recommend (sensibly) using
multiple observations to simultaneously fit for sky $\sky_i$ and
object $\fluxLSF_j$, but that detail is beyond the scope of this
summary.  They recommend smoothing the vectors $\fluxLSF_j$ to make
new vectors that have uncorrelated errors (diagonal covariance).  This
methodology is ``optimal'' in that the flux vectors $\fluxLSF_j$ are
the maximum-likelihood flux vectors given the pixel data $\pixels_i$;
the smoothing is just a linear method for handling the ``ringing''
that occurs because neighboring pixels tend to be strongly
anti-correlated in any simple point estimate of the flux.

DWH: Write the more important point that the chi-squared against the
smoothed flux vectors is close to identical to the chi-squared against
the raw pixels; in fact open with this if it is not totally clear in
the final version of Bolton \& Schlegel.

There are a number of issues with this method for optimal extraction.
One issue is that if extraction is asked to obey the rules of
probabilistic inference, it ought to return a (posterior) probability
distribution over spectra and not a single point estimate of each
spectrum.  The second is that the smoothing step is only the best
thing to do for certain very specific projects where the investigator
cannot carry forward covariance information, or the investigator is
uninterested in marginalization over uncertainties.  Neither of these
conditions is met for the general spectroscopic user, and in
particular for our purposes, neither is met for the investigators who
are expected to perform the precise measurements that form the primary
science goals of the BOSS survey.

Fortunately, when errors are normally distributed, if we also place
uninformative normal priors on the spectra, it is straightforward to
compute analytically the full posterior distribution for each spectrum
$\flux_j$: The posterior will be
\begin{equation}
p(\flux_j|\pixels_i,\pixelvariance_i,I) =
  \Normal(\flux_j|\fluxLSF_j,\covariance_{ij}) \quad,
\end{equation}
where we have assumed that the priors are almost totally
uninformative; in this case the least-squares solution becomes the
mean, and the least-squares covariance matrix becomes the variance of
the posterior, which is exactly a (high-dimensional) normal
distribution.  Note that the variance tensor $\covariance_{ij}$ will
\emph{not} in general be diagonal, although if the data are good, it
will be very sparse.

In detail, extraction ought to involve a very large fit, because when
spectra of different objects overlap on the spectroscopic image pixels
(as they always do at some level when a spectrograph is efficiently
designed), then bias, dark, sky, and all object spectra must be
estimated simultaneously from all science and calibration images taken
together.  For a typical BOSS observation (a set of exposures, each
with data from $10^3$ fibers), this is a simultaneous fit of about
$10^8$ pixels with a few $10^7$ parameters.  Of course when the errors
can be treated as Gaussian, this fit is convex and can be performed by
gradient or sparse methods even when the arrays are much too large to
fit in memory in any na\"ive form (for example, as we fit in
Padmanabhan et al).

One unresolved issue for the future is that the pixel noise is never
Gaussian, for two reasons.  The first is that the detector is an
electron counter, which is subject to Poisson noise (perhaps in
addition to other kinds of noise); this leads to a weak
non-Gaussianity.  The second is that pixels get hit by cosmic rays,
which create large positive deviations that are generally hard to
model in the extraction.  The standard is to identify and mask
cosmic-ray hits (manually set individual pixel variance elements
$\Sigma_{ik}$ to large values at pixels $k$ that are likely to have
been affected by cosmic-ray hits) in a prior step.

\paragraph{Priors on the matrices $\Amatrix_j$:}
Our weakest possible expectation about the matrices $\Amatrix_j$ is
that they be ``smooth'' in certain ways.  For example, we expect
neighboring matrix rows and columns to be similar:
\begin{eqnarray}\displaystyle
A_{jk\ell} & \approx & A_{jk[\ell+1]} \\
A_{jk\ell} & \approx & A_{j[k+1]\ell} \\
A_{jk\ell} & \approx & A_{j[k+n]\ell} \quad ,
\end{eqnarray}
where $n$ is some kind ``unfolding length'' of the pixels from
two dimensions (the CCD array) to one dimension (the column vector
$\pixels$); the idea is that pixel $k$ has neighbor $[k+1]$ in one
direction on the CCD, and $[k+n]$ in an orthogonal direction.  We
expect---in the BOSS spectrographs and many other situations of
integral-field or multi-object spectrographs---adjacent spectroscopic
apertures to be similar, that is:
\begin{equation}
A_{jk\ell} \approx A_{[j+1][k+\delta]\ell} \quad ,
\end{equation}
where $\delta$ is the pixel displacement (in the one-dimensional,
unwrapped pixel ordering) between adjacent spectra.  In general,
$\delta$ will depend on $j$.  We expect the vast majority of the
elements of the matrices $\Amatrix_j$ to be zero---that is, we expect
that most pixels $k$ are very far from the detector footprint of
spectrum $j$.  Finally, we expect every element of every vector and
matrix in our entire problem to be positive: There are only positive
fluxes, positive electron numbers in the CCD, and positive
relationships between them.

We have stronger expectations than these, however.  We expect each
matrix $\Amatrix_j$ to describe something like a one-dimensional
spectral trace along the detector, convolved with a spatially varying,
pixel-convolved point-spread function.  That is, we expect to be able
to model the matrix with parameters only of the one-dimensional curve
that describes the wavelength-dependence and position of the spectral
trace for aperture $j$ plus parameters of a point-spread function and
its (presumably slow) variations with detector position.  This model
ought to be a good description of the optical properties of the
spectrograph---if the trace and point-spread function are given
appropriate freedom---but it will not be able to capture significant
pixel-to-pixel variations in the CCD sensitivity.  If these are
significant, they will have to be modeled with additional parameters.
In addition, we expect the traces to be low-order curves, and we
expect the PSF to be non-negative, smooth, and centrally peaked.  All
our prior expectations ought to be encoded in prior probability
distribution functions for parameters, preferably in a form that
preserves all the benefits of having a convex, quadratic optimization
problem.

DWH: Get explicit about priors on traces and PSFs

In the above, we have---by omission really---assumed that the matrices
$\Amatrix_j$ are all identical for all the exposures under
consideration.  Of course we do expect some variations in these
matrices from exposure to exposure (from, for example, flexure in the
spectrographs and atmospheric transparency) and from month to month
(from, for example, detector decay, fiber damage, and dust build-up on
the mirrors).  We want to permit variation in the matrices but not
arbitrary variation.  That suggests going hierarchical?  Similarly,
the sky, flatfield lamps, and arc lamps will vary over time but again,
not arbitrarily.  Our challenge is to discover the variability and
then implement priors that once again respect convexity and quadratic
form.

\paragraph{Inferring the matrices $\Amatrix_j$:}
The expressions for the pixel values $p_{ik}$ are linear in the object
fluxes $f_{j\ell}$; this makes optimal extraction practical.  They are
also linear in the matrix elements $A_{jk\ell}$; this makes inference
of the matrices $\Amatrix_j$ practical, as we will demonstrate here.

\end{document}
